<!DOCTYPE html>
<html>
<head>
  <title>Unidad 4</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Unidad 4 Aspectos básicos de la computación paralela</h1>
      <nav class="navbar">
        <ul class="menu">
            <li>
              <a href="#4.1 Aspectos básicos de la computación paralela.">4.1 Aspectos básicos de la computación paralela </a>
            </li>
            
            <li>
              <a href="#4.2 Tipos de computación paralela.">4.2 Tipos de computación paralela.</a>
                <ul class="submenu">
                  <li><a href="#4.2.1 Clasificacion.">4.2.1 Clasificacion.</a></li>
                  <li><a href="#4.2.2 Arquitectura de computadores secuenciales.">4.2.2 Arquitectura de computadores secuenciales.</a></li>
                  <li><a href="#4.2.3 Organización de direcciones de memoria.">4.2.3 Organización de direcciones de memoria.</a></li>
                </ul>
            </li>
            <li>
              <a href="#4.3 Sistema de memoria compartida.">4.3 Sistema de memoria compartida.</a>
                        <ul class="submenu">
                            <li><a href="#4.3.1.1 Redes de medio compartida.">4.3.1.1 Redes de medio compartida.</a></li>
                            <li><a href="#4.3.1.2 Redes conmutadas.">4.3.1.2 Redes conmutadas.</a></li>
                        </ul> 
            </li>
            <li>
              <a href="4.4 Sistemas de memoria Distribuida">4.4 Sistemas de memoria Distribuida</a>
            </li>
            <li>
              <a href="#4.5 Casos de estudio.">4.5 Casos de estudio.</a>
            </li>
        </ul>
    </nav>
  </header>
  <br><br><br><br>
    <div class="info">
    <h2 id="4.1 Aspectos básicos de la computación paralela.">4.1 Aspectos básicos de la computación paralela </h2>
    <p>La computación paralela se basa en la idea de dividir un problema en tareas más pequeñas y procesarlas de manera simultánea<br>
      utilizando múltiples recursos de computación. <br>
      Esto permite un procesamiento más rápido y eficiente en comparación con los enfoques secuenciales tradicionales. <br>
      Algunos aspectos fundamentales de la computación paralela incluyen la sincronización de tareas, la comunicación entre procesos y la gestión de recursos.<br>
      Algunos aspectos básicos son:
      <ol>
        <li><b>División de tareas:<br></b>En lugar de realizar una tarea completa secuencialmente, se divide en partes más pequeñas que pueden ejecutarse simultáneamente en diferentes núcleos de procesador o en diferentes máquinas.</li>
        <li><b>Coordinación:<br></b> Para que la computación paralela sea efectiva, es necesario coordinar la ejecución de las tareas individuales y combinar los resultados de manera adecuada al finalizar.

        </li>
        <li><b>Arquitecturas paralelas:<br></b>Hay diferentes arquitecturas paralelas, desde sistemas multiprocesador en un solo chip hasta clústeres de computadoras interconectadas. Cada una tiene sus propias ventajas y desafíos.</li>
        <li><b>Programación paralela: <br></b>Para aprovechar al máximo la computación paralela, los programas deben estar diseñados específicamente para distribuir tareas y sincronizar la ejecución de manera eficiente. Esto puede involucrar el uso de bibliotecas de programación paralela como OpenMP, MPI (Message Passing Interface) o CUDA (para GPU).</li>
      </ol>

    </p>
    <div class="center">
              <img src="Media/Img/computacion paralela.jpg">
      </div>
    <h2 id="4.2 Tipos de computación paralela.">4.2 Tipos de computación paralela.</h2>
    <p>Existen varios tipos de computación paralela que se utilizan en diferentes contextos y escenarios. Algunos de los enfoques más comunes incluyen el procesamiento paralelo a nivel de bit, a nivel de instrucción, a nivel de datos y a nivel de tarea. Estos enfoques se diferencian en cómo se dividen y procesan las tareas y los datos.<br>
      <ol>
        <li><b>Paralelismo a nivel de datos (DLP - Data-Level Parallelism):<br></b>
          <ul>
            <li>En este enfoque, se dividen los datos en partes más pequeñas y se procesan simultáneamente utilizando varios recursos computacionales.</li>
            <li>Se utiliza en operaciones intensivas en datos como el procesamiento de imágenes, el procesamiento de señales digitales y la manipulación de matrices en álgebra lineal.</li>
          </ul></li>
        <li><b>Paralelismo a nivel de tarea (TLP - Task-Level Parallelism):<br></b>
          <ul>
            <li>En este caso, se dividen las tareas en subconjuntos más pequeños que pueden ejecutarse simultáneamente.</li>
            <li>Es útil para aplicaciones que consisten en múltiples tareas independientes entre sí, como aplicaciones de servidor web, procesamiento por lotes y simulaciones de eventos discretos.</li>
          </ul>
        </li>
        <li><b>Paralelismo a nivel de instrucción (ILP - Instruction-Level Parallelism):<br></b>
        <ul>
          <li>Este tipo de paralelismo implica la ejecución simultánea de múltiples instrucciones en una sola tarea o hilo de ejecución.</li>
          <li>Se logra mediante técnicas como la ejecución fuera de orden, la ejecución especulativa y la segmentación de instrucciones.</li>
          <li>Se encuentra comúnmente en procesadores superescalares y VLIW (Very Long Instruction Word).</li>
        </ul></li>
        <li><b>Paralelismo a nivel de bucle (Loop-Level Parallelism):<br></b>
        <ul>
          <li>Se centra en la división de bucles en tareas más pequeñas que pueden ejecutarse simultáneamente.</li>
          <li>Es fundamental en la optimización de código para arquitecturas paralelas y se puede lograr mediante técnicas como la vectorización, la paralelización automática y la ejecución fuera de orden.</li>
        </ul></li>
        <li><b>Paralelismo a nivel de datos y tareas combinado:<br></b>
        <ul>
          <li>Muchas aplicaciones aprovechan tanto el paralelismo a nivel de datos como el paralelismo a nivel de tarea para optimizar el rendimiento.</li>
          <li>Este enfoque se utiliza en una amplia gama de aplicaciones, desde la computación científica hasta el procesamiento de transacciones en bases de datos distribuidas.</li>
        </ul></li>
        <li><b>Paralelismo a nivel de sistema (System-Level Parallelism):<br></b>
        <ul>
          <li>Se refiere a la distribución de tareas y recursos a través de múltiples sistemas o nodos de procesamiento.</li>
          <li>Es fundamental en la construcción de sistemas distribuidos y en la computación en la nube, donde múltiples servidores colaboran para realizar tareas complejas.</li>
        </ul></li>
      </ol>
    </p>
    <h2 id="4.2.1 Clasificacion.">4.2.1 Clasificacion.</h2>
    <p>La clasificación de la computación paralela puede realizarse en función de la forma en que se dividen las tareas y los datos, así como de la forma en que se coordinan y comunican los procesos paralelos. Algunas clasificaciones comunes incluyen la computación paralela a nivel de bit, a nivel de instrucción, a nivel de datos y a nivel de tarea.
      <ul>
        <li><b>Computación multinúcleo:<br></b>Un procesador multinúcleo es un procesador que incluye múltiples unidades de ejecución (núcleos) en el mismo chip. Un procesador multinúcleo puede ejecutar múltiples instrucciones por ciclo de secuencias de instrucciones múltiples.</li>
        <li><b>Multiprocesamiento simétrico:<br></b>Un multiprocesador simétrico (SMP) es un sistema computacional con múltiples procesadores idénticos que comparten memoria y se conectan a través de un bus. La contención del bus previene el escalado de esta arquitectura.</li>
        <li><b>Computación en cluster:<br></b>Un clúster es un grupo de ordenadores débilmente acoplados que trabajan en estrecha colaboración, de modo que en algunos aspectos pueden considerarse como un solo equipo.</li>
        <li><b>Procesamiento paralelo masivo:<br></b>ienden a ser más grandes que los clústeres, con «mucho más» de 100 procesadores. En un MPP, cada CPU tiene su propia memoria y una copia del sistema operativo y la aplicación.</li>
        <li><b>Computación Distribuida:<br></b>La computación distribuida es la forma más distribuida de la computación paralela. Se hace uso de ordenadores que se comunican a través de la Internet para trabajar en un problema dado.</li>
        <li><b>Computadoras Paralelas Especializadas:<br></b>Dentro de la computación paralela, existen dispositivos paralelos especializados que generan interés. Aunque no son específicos para un dominio, tienden a ser aplicables sólo a unas pocas clases de problemas paralelos.</li>
        <li><b>Cómputo Reconfigurable con Arreglos de Compuertas Programables:<br></b>
          El cómputo reconfigurable es el uso de un arreglo de compuertas programables (FPGA) como coprocesador de un ordenador de propósito general.</li>
        <li><b>Cómputo de propósito general en unidades de procesamiento gráfico:<br></b>Es una tendencia relativamente reciente en la investigación de ingeniería informática. Los GPUs son co-procesadores que han sido fuertemente optimizados para procesamiento de gráficos por computadora.</li>
        <li><b>Circuitos Integrados de Aplicación Específica:<br></b>Debido a que un ASIC (por definición) es específico para una aplicación dada, puede ser completamente optimizado para esa aplicación. Como resultado, para una aplicación dada, un ASIC tiende a superar a un ordenador de propósito general.</li>
        <li><b>Procesadores Vectoriales:<br></b>Pueden ejecutar la misma instrucción en grandes conjuntos de datos. Tienen operaciones de alto nivel que trabajan sobre arreglos lineales de números o vectores.</li>

      </ul>

    </p>
    <h2 id="4.2.2 Arquitectura de computadores secuenciales.">4.2.2 Arquitectura de computadores secuenciales.</h2>
    <p>La arquitectura de computadores secuencial se refiere a los sistemas informáticos tradicionales en los que las instrucciones se ejecutan una tras otra en secuencia. Este tipo de arquitectura sigue siendo común en muchas computadoras personales y estaciones de trabajo.
      El sistema secuencial requiere de la utilización de un dispositivo de memoria que pueda almacenar la historia pasada de sus entradas (denominadas variables de estado) y le permita mantener su estado durante algún tiempo, estos dispositivos de memoria pueden ser sencillos como un simple retardador o celdas de memoria de tipo DRAM, SRAM o multivibradores biestables también conocido como Flip-Flop.
    </p>
    <h2 id="4.2.3 Organización de direcciones de memoria.">4.2.3 Organización de direcciones de memoria.</h2>
    <p>La organización de direcciones de memoria se refiere a cómo se asignan y acceden a las direcciones de memoria en un sistema de computación paralela. Esto incluye consideraciones como la memoria compartida, la memoria distribuida y las técnicas de direccionamiento utilizadas para acceder a los datos en paralelo.</p>
    <h2 id="4.3 Sistema de memoria compartida.">4.3 Sistema de memoria compartida.</h2>
    <p>Los sistemas de memoria compartida son un enfoque de computación paralela en el que múltiples procesadores acceden a una misma área de memoria compartida. Esto permite a los procesadores compartir datos y comunicarse de manera eficiente. Dentro de los sistemas de memoria compartida, existen dos tipos principales de redes: las redes de medio compartida y las redes conmutadas.
      <br>La mayoría de los multiprocesadores comerciales son del tipo UMA (Uniform Memory Access): todos los procesadores tienen igual tiempo de acceso a la memoria compartida. En la arquitectura UMA los procesadores se conectan a la memoria a través de un bus, una red multietapa o un conmutador de barras cruzadas (red multietapa o un conmutador de barras cruzadas (crossbar crossbar) y disponen de su propia ) y disponen de su propia memoria caché. Los procesadores tipo NUMA (Non Uniform Memory Access) presentan tiempos de acceso a la memoria compartida que dependen de la ubicación del elemento de proceso y la memoria.

    </p>
    <h2 id="4.3.1.1 Redes de medio compartida.">4.3.1.1 Redes de medio compartida.</h2>
    <p>Las redes de medio compartida son un tipo de arquitectura de memoria compartida en la que los procesadores se conectan físicamente a un bus compartido o a una red de interconexión. Los procesadores pueden leer y escribir en la memoria compartida a través de este medio compartido.</p>
    <h2 id="4.3.1.2 Redes conmutadas.">4.3.1.2 Redes conmutadas.</h2>
    <p>Las redes conmutadas, por otro lado, utilizan interruptores o conmutadores para establecer conexiones entre los procesadores y la memoria compartida. Estas redes ofrecen una mayor escalabilidad y capacidad de comunicación en comparación con las redes de medio compartida.</p>
    <div class="center">
              <img src="Media/Img/Red conmutada.png">
      </div>
    <h2 id="4.4 Sistemas de memoria Distribuida">4.4 Sistemas de memoria Distribuida</h2>
    <p>La Distributed SharedMemory (DSM, o memoria
      distribuida compartida) es un tipo de implementación
      hardware y software, en la que cada nodo de un cluster
      tiene acceso a una amplia memoria compartida que se
      añade a la memoria limitada privada, no compartida,
      propia de cada nodo.</p>
    <h2 id="4.5 Casos de estudio.">4.5 Casos de estudio.</h2>
    <p>
      El procesamiento distribuido se ha convertido en un área de gran importancia e interés dentro de la ciencia de la computación, produciendo profundas transformaciones en las líneas de investigación y desarrollo.
Interesa realizar investigación en la especificación, transformación, optimización y evaluación de algoritmos distribuidos y paralelos. Esto incluye el diseño y desarrollo de sistemas paralelos, la transformación de algoritmos secuenciales en paralelos, y las métricas de evaluación de performance sobre distintas plataformas de soporte (hardware y software).
      <br>Las lineas de investigacion y desarrollo son:<br>
      <ul>
        <li><b>Paralelización de algoritmos secuenciales. Diseño y optimización de algoritmos.</b></li>
        <li><b>Arquitecturas multicore y multithreading en multicore.</b></li>
        <li><b>Modelos de representación y predicción de performance de algoritmos paralelos.</b></li>
        <li><b>Mapping y scheduling de aplicaciones paralelas sobre distintas arquitecturas multiprocesador.</b></li>
        <li><b>Métricas del paralelismo. Speedup, eficiencia, rendimiento, granularidad, superlinealidad.</b></li>
        <li><b>Balance de carga estático y dinámico. Técnicas de balanceo de carga.</b></li>
        <li><b>Análisis de los problemas de migración y asignación óptima de procesos y datos a procesadores.</b></li>
        <li><b>Patrones de diseño de algoritmos paralelos.</b></li>
        <li><b>Escalabilidad de algoritmos paralelos en arquitecturas multiprocesador distribuidas.</b></li>
        <li><b>Implementación de soluciones sobre diferentes modelos de arquitectura homogéneas y heterogéneas.</b></li>
        <li><b>Laboratorios remotos para el acceso transparente a recursos de cómputo paralelo.</b></li>

      </ul>
    </p>
     <div class="buttons">
      <button onclick="window.location.href = 'index.html';">Menu</button>
        <button onclick="window.location.href = 'Unidad3.html';">Anterior</button>
        <button onclick="window.location.href = 'Practicas.html';">Siguiente</button>
      </div>
    </div>
</body>

<!-- Pie de Página -->
  <footer>
    <h6>&copy; <span id="currentYear"></span> Hugo Emilio Espinoza Tun</h6>
  </footer>

  <script>
    // Configurar el año actual en el pie de página
    document.getElementById('currentYear').innerText = new Date().getFullYear();
  </script>

</html>